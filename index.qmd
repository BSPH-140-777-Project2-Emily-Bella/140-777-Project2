---
title: "Project 2"
author: 
  - name: Emily Potts (epotts@jh.edu / epotts5) and Bella Satpathy-Horton (ghorton2@jh.edu / ghorton2)
    affiliation: Department of Biostatistics, Johns Hopkins
description: "Practicing functional programming and data collection paradigms"
date: 2024-11-10
format:
  html:
    code-fold: true
---

# Background

**Due date: November 15 at 11:59pm**

The goal of this assignment is to practice some of the skills we have been learning about in class around data collection paradigms and functional programming.

### To submit your project

You need to create a private GitHub Classroom repository (only one per group) for you and your partner, which will be posted in CoursePlus. This creates an empty GitHub repository. You need to show all your code and submit both the `.qmd` file and the rendered HTML file. Please include section headers for each of the components below. All plots should have titles, subtitles, captions, and human-understandable axis labels. The TAs will grade the contents in the GitHub Classroom repo by cloning the repo and checking for all the things described below.

Because you will work with a partner, **please be sure to include the names, emails, and JHED IDs for both individuals in your submitted work**.

# Part 1

Here, you and your partner will practice using a API and making data visualizations.

The API we will use is `tidycensus` (<https://walker-data.com/tidycensus>), which is an R package that allows users to interface with a select number of the US Census Bureau’s data APIs and return tidyverse-ready data frames, optionally with simple feature geometry included.

The goal of this part is to create a set of data visualizations using the US Census Bureau's data.

::: callout-important
To use this API, you must obtain an API key, which can be found at the top of this page:

-   <https://walker-data.com/tidycensus/articles/basic-usage>
:::

::: callout-tip
When you use an API, you want to figure out the data you want to extract and then save it locally so that you are not using the API each time you knit or render your data analysis.

Most APIs have limits on the number of times you can ping it in a given hour and your IP address can be blocked if you try to ping it too many times within a short time.

Therefore, it is strongly suggested that you create a new folder called `data` locally and save the output from extracting data from the `tidycensus` API. In this way, you can read the data in locally each time you knit/render the document, rather than continue to pull data from the API each time you knit/render the document.
:::

1.  Choose a question to investigate. Describe what is the question you aim to answer with the data and what you want to visualize.

This brief analysis is meant to examine the prevalence of disability status across census tracts within Pennsylvania, specifically focusing on stratification by age and sex and their overlap.

2.  Extract data from the `tidycensus` API. Use at least three different calls to the `tidycensus` API to extract out different datasets. For example, these could be across years, locations, or variables.

```{r}
#|eval: false

library(tidycensus)
library(tidyverse)

census_api_key("7d55de6317ef97afe80c744aab78d1b75e2d1970")

#get the variables 
data_1 = get_acs(geography = "tract", year = 2017, variables = c(male_under5 = "B18101_003", male_under5_disability = "B18101_004", male_1517 = "B18101_006", male_1517_disability = "B18101_007", male_1834 = "B18101_009", male_1834_disability = "B18101_010"), state = "Pennsylvania")
data_2 = get_acs(geography = "tract", year = 2017, variables = c(male_3564 = "B18101_012", male_3564_disability = "B18101_013", male_6574 = "B18101_015", male_6574_disability = "B18101_016", male_75over = "B18101_018", male_75over_disability = "B18101_019"), state = "Pennsylvania")
data_3 = get_acs(geography = "tract", year = 2017, variables = c(female_under5 = "B18101_022", female_under5_disability = "B18101_023", female_1517 = "B18101_025", female_1517_disability = "B18101_026", female_1834 = "B18101_028", female_1834_disability = "B18101_029"), state = "Pennsylvania")
data_4 = get_acs(geography = "tract", year = 2017, variables = c(female_3564 = "B18101_031", female_3564_disability = "B18101_032", female_6574 = "B18101_034", female_6574_disability = "B18101_035", female_75over = "B18101_037", female_75over_disability = "B18101_038"), state = "Pennsylvania")
data_5 = get_acs(geography = "tract", year = 2017, variables = c(total = "B18101_001", total_male = "B18101_002", total_female = "B18101_021"), state = "Pennsylvania")

#pivot into columns
data_1 = data_1 %>% 
  pivot_wider(names_from = variable, values_from = c(estimate, moe), names_sep = ".")
data_2 = data_2 %>% 
  pivot_wider(names_from = variable, values_from = c(estimate, moe), names_sep = ".")
data_3 = data_3 %>% 
  pivot_wider(names_from = variable, values_from = c(estimate, moe), names_sep = ".")
data_4 = data_4 %>% 
  pivot_wider(names_from = variable, values_from = c(estimate, moe), names_sep = ".")
data_5 = data_5 %>% 
  pivot_wider(names_from = variable, values_from = c(estimate, moe), names_sep = ".")

#join into one data set
data = list(data_1, data_2, data_3, data_4, data_5) %>% 
  reduce(full_join, by = c("GEOID", "NAME"))

#save data locally
write_csv(data, "project2_datafixed.csv")
```

3.  Clean the data. Include some form of data wrangling and data visualization using packages such as `dplyr` or `tidyr`. Other packages that might be helpful to you include `lubridate`, `stringr`, and `forcats`. You **must use at least two functions from `purrr`**.

```{r}
#data wrangling into desired format
data = data %>% 
  mutate(
    percent_male_under5 = estimate.male_under5_disability/estimate.male_under5,
    percent_male_1517 = estimate.male_1517_disability/estimate.male_1517,
    percent_male_1834 = estimate.male_1834_disability/estimate.male_1834,
    percent_male_3564 = estimate.male_3564_disability/estimate.male_3564,
    percent_male_6574 = estimate.male_6574_disability/estimate.male_6574,
    percent_male_75over = estimate.male_75over_disability/estimate.male_75over,
    percent_female_under5 = estimate.female_under5_disability/estimate.female_under5,
    percent_female_1517 = estimate.female_1517_disability/estimate.female_1517,
    percent_female_1834 = estimate.female_1834_disability/estimate.female_1834,
    percent_female_3564 = estimate.female_3564_disability/estimate.female_3564,
    percent_female_6574 = estimate.female_6574_disability/estimate.female_6574,
    percent_female_75over = estimate.female_75over_disability/estimate.female_75over
  ) %>% 
  mutate(
    percent.male = estimate.total_male/estimate.total,
    percent.female = estimate.total_female/estimate.total
  )

data_full = data

data = data[,57:70]

#initial visualization
data %>% 
  map(summary)
```

https://walker-data.com/census-r/an-introduction-to-tidycensus.html might be helpful

4.  Visualize the data. Create data visualizations of your choice. However, your analysis should include at least three plots with you using at least two different `geom_*()` functions from `ggplot2` (or another package with `geom_*()` functions).

```{r}
#|warning: false

ggplot(data, aes(percent_female_under5))+geom_histogram(color = "purple")+ggtitle("Percent Females Under 5 with Disability")
ggplot(data, aes(percent_male_under5))+geom_histogram(color = "green")+ggtitle("Percent Males Under 5 with Disability")

ggplot(data, aes(percent_female_1517))+geom_histogram(color = "purple")+ggtitle("Percent Females 5-17 with Disability")
ggplot(data, aes(percent_male_1517))+geom_histogram(color = "green")+ggtitle("Percent Males 5-17 with Disability")

ggplot(data, aes(percent_female_1834))+geom_histogram(color = "purple")+ggtitle("Percent Females 18-34 with Disability")
ggplot(data, aes(percent_male_1834))+geom_histogram(color = "green")+ggtitle("Percent Males 18-34 with Disability")

ggplot(data, aes(percent_female_3564))+geom_histogram(color = "purple")+ggtitle("Percent Females 35-64 with Disability")
ggplot(data, aes(percent_male_3564))+geom_histogram(color = "green")+ggtitle("Percent Males 35-64 with Disability")

ggplot(data, aes(percent_female_6574))+geom_histogram(color = "purple")+ggtitle("Percent Females 65-74 with Disability")
ggplot(data, aes(percent_male_6574))+geom_histogram(color = "green")+ggtitle("Percent Males 65-74 with Disability")

ggplot(data, aes(percent_female_75over))+geom_histogram(color = "purple")+ggtitle("Percent Females Over 75 with Disability")
ggplot(data, aes(percent_male_75over))+geom_histogram(color = "green")+ggtitle("Percent Males Over 75 with Disability")
```

```{r}
#|warning: false

ggplot()+geom_point(data, mapping = aes(x = percent_male_under5, y=percent_female_under5), color = "red")+geom_point(data, mapping = aes(x = percent_male_1517, y=percent_female_1517), color = "orange")+xlab("Percent Males with Disability")+ylab("Percent Females with Disability")+ggtitle("Males vs. Females Affected by Disability Under 5 and 5-17")

ggplot()+geom_point(data, mapping = aes(x = percent_male_1834, y=percent_female_1834), color = "green")+geom_point(data, mapping = aes(x = percent_male_3564, y=percent_female_3564), color = "blue")+xlab("Percent Males with Disability")+ylab("Percent Females with Disability")+ggtitle("Males vs. Females Affected by Disability 18-34 and 35-64")

ggplot()+geom_point(data, mapping = aes(x = percent_male_6574, y=percent_female_6574), color = "purple")+geom_point(data, mapping = aes(x = percent_male_75over, y=percent_female_75over), color = "gray")+xlab("Percent Males with Disability")+ylab("Percent Females with Disability")+ggtitle("Males vs. Females Affected by Disability 65-74 and 75+")
```

5.  Report your findings. Provide a paragraph summarizing your methods and key findings. Include any limitations or potential biases in pulling data from the API or the analysis. Be sure to comment and organize your code so is easy to understand what you are doing.

Overall, pulling data from the API was a difficult process that limited the extent of the analysis that could be done within this time frame. Because each separate variable represents the stratification across multiple groupings, selecting the appropriate variables and then transforming them into meaningful variables is a barrier. That being said, I was able to do a short analysis looking at age and sex in relation to disability status. Unsurprisingly, the distribution of disability prevalence shifted across the life span, becoming more and more common as the population aged. The distributions generated by the set of histograms due not seem to indicate any great differences by sex. This last finding was mostly corroborated by the scatter plots comparing the prevalence of disability within the age groups between males and females. In the first plot, disability in those under 5 (red) and in those between 5 and 17 (orange) is an uncommon finding, and as such, the data does not coalesce into an recognizable pattern. In the second plot, as age increases and disability becomes more prominent, the data starts to coalesce into a recognizable line - indicating equal percentages affected by disability between sexes in the 18-34 group (green) and the 35-64 group (blue). This clear delineation starts to fall away with the 65-74 age groups (purple) and the 75+ age groups (gray), although this might be attributable to differences in life expectancy between sexes. Notably, these are all initial findings and are limited in scope and analysis, but could lay the groundwork for some interesting future exploration.

# Part 2

In this project, we perform web scraping on the IMDb website to gather and analyze movie information from their list of Most Popular Movies (100 movies across all genres). Specifically, this analysis seeks to compare characteristics across three genres: Sci-Fi, Adventure, and Mystery. Utilizing the `rvest` package, we extract structured data from each genre’s IMDb chart page and compile the results into a unified data frame, imdb_data, for further analysis.

We begin by defining vectors containing the URLs and their corresponding genre labels for Sci-Fi, Adventure, and Mystery. This setup allows us to systematically associate each URL with its respective genre. Next, we initialize an empty data frame, imdb_data, with columns for Title, Duration, Rating, Reviews, Year, and Genre. This structure ensures that all collected data is organized consistently and that text-based fields remain in character format, facilitating easier data manipulation.

The script involves looping through each URL and its associated genre. Within each iteration, we use `rvest` functions to scrape specific data elements from the webpage. We extract movie titles using the CSS selector .with-margin .ipc-title__text, durations with .cli-title-metadata-item:nth-child(2), ratings via .ipc-rating-star--rating, review counts through .ipc-rating-star--voteCount, and release years using .cli-title-metadata-item:nth-child(1). For each of these attributes, we employ html_nodes() to target the desired HTML elements and html_text(trim = TRUE) to retrieve and clean the text data.

To ensure consistency in our data, we address potential discrepancies in the length of the scraped vectors by padding shorter vectors with NA values based on the longest vector length for each genre’s page. We then create a temporary data frame, temp_data, that includes the scraped data along with the corresponding genre label repeated for each entry. This temporary data frame is appended to our main imdb_data data frame using bind_rows(), resulting in a comprehensive dataset that includes a Genre column for easy categorization.
```{r, message = FALSE}
library(rvest)
library(tidyverse)
library(dplyr)
library(ggplot2)
```

```{r}
# Define the URLs and their corresponding genres
urls = c(
  "https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm&sort=rank%2Casc&genres=sci-fi",
  "https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm&sort=rank%2Casc&genres=adventure",
  "https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm&sort=rank%2Casc&genres=mystery"
)
genres = c("Sci-Fi", "Adventure", "Mystery")

# Initialize an empty data frame to store results
imdb_data = data.frame(
  Title = character(),
  Duration = character(),
  Rating = character(),
  Reviews = character(),
  Year = character(),
  Genre = character(),
  stringsAsFactors = FALSE
)

# Loop through each URL and genre
for (i in seq_along(urls)) {
  # Scrape the webpage
  url = urls[i]
  genre = genres[i]
  webpage = read_html(url)
  
  # Extract movie titles
  titles = webpage %>%
    html_nodes(".with-margin .ipc-title__text") %>%
    html_text(trim = TRUE)
  
  # Extract movie durations
  durations = webpage %>%
    html_nodes(".cli-title-metadata-item:nth-child(2)") %>%
    html_text(trim = TRUE)
  
  # Extract movie ratings
  ratings = webpage %>%
    html_nodes(".ipc-rating-star--rating") %>%
    html_text(trim = TRUE)
  
  # Extract number of reviews
  reviews = webpage %>%
    html_nodes(".ipc-rating-star--voteCount") %>%
    html_text(trim = TRUE)
  
  # Extract year
  years = webpage %>%
    html_nodes(".cli-title-metadata-item:nth-child(1)") %>%
    html_text(trim = TRUE)
  
  # Ensure all vectors have the same length by padding with NA
  max_length = max(length(titles), length(durations), length(ratings), length(reviews), length(years))
  titles = c(titles, rep(NA, max_length - length(titles)))
  durations = c(durations, rep(NA, max_length - length(durations)))
  ratings = c(ratings, rep(NA, max_length - length(ratings)))
  reviews = c(reviews, rep(NA, max_length - length(reviews)))
  years = c(years, rep(NA, max_length - length(years)))
  
  # Combine the data into a temporary data frame with Genre
  temp_data = data.frame(
    Title = titles,
    Duration = durations,
    Rating = ratings,
    Reviews = reviews,
    Year = years,
    Genre = rep(genre, max_length), # Add genre label here
    stringsAsFactors = FALSE
  )
  
  # Append the temporary data frame to the main data frame
  imdb_data = bind_rows(imdb_data, temp_data)
}

# Display the combined data with genre labels
head(imdb_data)
```

Following the data collection phase, we proceed with cleaning and transforming the data using `dplyr` and `tidyr` functions. We convert review counts from shorthand notations (e.g., “39K”) to full numerical values (e.g., “39000”) by removing any parentheses and expanding “K” or “M” suffixes to represent thousands or millions, respectively. Additionally, we transform the Duration field from an "hours and minutes" format into total minutes. This is achieved by splitting the duration string into separate hours and minutes components using the separate() function, converting these components to numeric values, and then calculating the total duration in minutes by combining them. We also needed to manually shift observations based on missingness for Reviews and Ratings for two films.
```{r}
# Convert Duration and Reviews
df = imdb_data %>%
  mutate(Reviews = gsub("^\\(|\\)$", "", Reviews)) %>%
  mutate(Reviews = ifelse(grepl("\\.\\d+K$", Reviews),
                          paste0(gsub("\\.", "", sub("K$", "", Reviews)), "00"),
                          Reviews)) %>%
  mutate(Reviews = ifelse(grepl("K$", Reviews),
                          paste0(sub("K$", "", Reviews), "000"),
                          Reviews)) %>%
  mutate(Reviews = ifelse(grepl("\\.\\d+M$", Reviews),
                          paste0(gsub("\\.", "", sub("M$", "", Reviews)), "00000"),
                          Reviews)) %>%
  separate(Duration, into = c("Hours", "Minutes"), sep = "h ", remove = FALSE) %>%
  mutate(Hours = as.numeric(Hours),
         Minutes = as.numeric(sub("m", "", Minutes))) %>%
  separate(Duration, into = c("Hours", "Minutes"), sep = "h ", remove = FALSE) %>%
  mutate(Hours = as.numeric(Hours),
         Minutes = as.numeric(sub("m", "", Minutes)),
         Minutes2 = Hours * 60,
         Duration = Minutes + Minutes2) %>%
  select(-Hours, -Minutes, -Minutes2) %>% 
  mutate(Reviews = as.numeric(Reviews)) %>% 
  mutate(Rating = as.numeric(Rating)) 

df$Rating[51:61] = c(NA, df$Rating[51:60])
df$Rating[38:43] = c(NA, df$Rating[38:42])
df$Reviews[51:61] = c(NA, df$Reviews[51:60])
df$Reviews[38:43] = c(NA, df$Reviews[38:42])

head(df)
```

We produce a violin plot showing the differences in rating distributions for each genre. Adventure and Sci-Fi movies generally have higher ratings clustered between 6 and 8, with Sci-Fi showing the widest and most even spread across this range. However, Mystery movies display a much denser concentration around the 6–8 range with fewer high ratings. This suggests that this genre might have more consistency but slightly lower average IMDb ratings compared to the other two. To complement the graph, it is also interesting to compare the means across the genres in which the Adventure genre has the highest mean rating of 7.26, while Mystery and Sci-Fi are around 6.7-6.8. 

```{r, warning = FALSE}
# Create a violin plot of movie rating by genre
ggplot(df, aes(x = Genre, y = Rating)) +
  geom_violin(trim = FALSE, fill = "skyblue") +
  labs(
    title = "Distribution of IMDb Ratings Across Genres",
    subtitle = "Comparing Adventure, Mystery, and Sci-Fi movie ratings on IMDb",
    x = "Movie Genre",
    y = "IMDb Rating",
    caption = "Data Source: IMDb"
  ) +
  theme_minimal()

aggregate(Rating ~ Genre, data = df, FUN = mean)
```

A limitation of this analysis is that we were unable to retrieve the Popularity ranking (1-100) from the website due to a lack of CSS tag, which would have been interesting to compare to the Rating. There is also the issue of missing data for a few films, which meant I had to manually adjust the data to reflect the website. Hence, this analysis would be difficult to scale to a larger dataset or to include more genres.