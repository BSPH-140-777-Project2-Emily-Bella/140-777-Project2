---
title: "Project 2"
author: 
  - name: Emily Potts (epotts@jh.edu / epotts5) and Bella Satpathy-Horton (ghorton2@jh.edu / ghorton2)
    affiliation: Department of Biostatistics, Johns Hopkins
description: "Practicing functional programming and data collection paradigms"
date: 2024-11-10
knitr: true
---

# Background

**Due date: November 15 at 11:59pm**

The goal of this assignment is to practice some of the skills we have been learning about in class around data collection paradigms and functional programming.

### To submit your project

You need to create a private GitHub Classroom repository (only one per group) for you and your partner, which will be posted in CoursePlus. This creates an empty GitHub repository. You need to show all your code and submit both the `.qmd` file and the rendered HTML file. Please include section headers for each of the components below. All plots should have titles, subtitles, captions, and human-understandable axis labels. The TAs will grade the contents in the GitHub Classroom repo by cloning the repo and checking for all the things described below.

Because you will work with a partner, **please be sure to include the names, emails, and JHED IDs for both individuals in your submitted work**.

# Part 1

Here, you and your partner will practice using a API and making data visualizations.

The API we will use is `tidycensus` (<https://walker-data.com/tidycensus>), which is an R package that allows users to interface with a select number of the US Census Bureauâ€™s data APIs and return tidyverse-ready data frames, optionally with simple feature geometry included.

The goal of this part is to create a set of data visualizations using the US Census Bureau's data.

::: callout-important
To use this API, you must obtain an API key, which can be found at the top of this page:

-   <https://walker-data.com/tidycensus/articles/basic-usage>
:::

::: callout-tip
When you use an API, you want to figure out the data you want to extract and then save it locally so that you are not using the API each time you knit or render your data analysis.

Most APIs have limits on the number of times you can ping it in a given hour and your IP address can be blocked if you try to ping it too many times within a short time.

Therefore, it is strongly suggested that you create a new folder called `data` locally and save the output from extracting data from the `tidycensus` API. In this way, you can read the data in locally each time you knit/render the document, rather than continue to pull data from the API each time you knit/render the document.
:::

1.  Choose a question to investigate. Describe what is the question you aim to answer with the data and what you want to visualize.

Education levels in urban vs rural PA?

2.  Extract data from the `tidycensus` API. Use at least three different calls to the `tidycensus` API to extract out different datasets. For example, these could be across years, locations, or variables.

```{r}
library(tidycensus)
library(tidyverse)

census_api_key("7d55de6317ef97afe80c744aab78d1b75e2d1970")
```

3.  Clean the data. Include some form of data wrangling and data visualization using packages such as `dplyr` or `tidyr`. Other packages that might be helpful to you include `lubridate`, `stringr`, and `forcats`. You **must use at least two functions from `purrr`**.

https://walker-data.com/census-r/an-introduction-to-tidycensus.html might be helpful

4.  Visualize the data. Create data visualizations of your choice. However, your analysis should include at least three plots with you using at least two different `geom_*()` functions from `ggplot2` (or another package with `geom_*()` functions).

5.  Report your findings. Provide a paragraph summarizing your methods and key findings. Include any limitations or potential biases in pulling data from the API or the analysis. Be sure to comment and organize your code so is easy to understand what you are doing.

# Part 2

In this part, you and your partner will use the `rvest` package to scrape data from a website, wrangle and analyze the data, and summarize your findings.

1.  Choose a website to scrape. Select a website with structured data in HTML tables or well-defined sections. Some examples could include:

IMDb scraping genre, duration, movie titles, reviews, ratings, and release years

2.  Extract data with `rvest`. Here, you will want to identify the specific HTML elements or CSS selectors containing the data. Then, use `rvest` functions like `read_html()`, `html_nodes()`, and `html_text()` or `html_table()` to retrieve the data.

```{r}
library(rvest)
library(tidyverse)
library(dplyr)
```


```{r}
# Define the URLs and their corresponding genres
urls <- c(
  "https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm&sort=rank%2Casc&genres=sci-fi",
  "https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm&sort=rank%2Casc&genres=adventure",
  "https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm&sort=rank%2Casc&genres=mystery"
)
genres <- c("Sci-Fi", "Adventure", "Mystery")

# Initialize an empty data frame to store results
imdb_data <- data.frame(
  Title = character(),
  Duration = character(),
  Rating = character(),
  Reviews = character(),
  Year = character(),
  Genre = character(),
  stringsAsFactors = FALSE
)

# Loop through each URL and genre
for (i in seq_along(urls)) {
  # Scrape the webpage
  url <- urls[i]
  genre <- genres[i]
  webpage <- read_html(url)
  
  # Extract movie titles
  titles <- webpage %>%
    html_nodes(".with-margin .ipc-title__text") %>%
    html_text(trim = TRUE)
  
  # Extract movie durations
  durations <- webpage %>%
    html_nodes(".cli-title-metadata-item:nth-child(2)") %>%
    html_text(trim = TRUE)
  
  # Extract movie ratings
  ratings <- webpage %>%
    html_nodes(".ipc-rating-star--rating") %>%
    html_text(trim = TRUE)
  
  # Extract number of reviews
  reviews <- webpage %>%
    html_nodes(".ipc-rating-star--voteCount") %>%
    html_text(trim = TRUE)
  
  # Extract year
  years <- webpage %>%
    html_nodes(".cli-title-metadata-item:nth-child(1)") %>%
    html_text(trim = TRUE)
  
  # Ensure all vectors have the same length by padding with NA
  max_length <- max(length(titles), length(durations), length(ratings), length(reviews), length(years))
  titles <- c(titles, rep(NA, max_length - length(titles)))
  durations <- c(durations, rep(NA, max_length - length(durations)))
  ratings <- c(ratings, rep(NA, max_length - length(ratings)))
  reviews <- c(reviews, rep(NA, max_length - length(reviews)))
  years <- c(years, rep(NA, max_length - length(years)))
  
  # Combine the data into a temporary data frame with Genre
  temp_data <- data.frame(
    Title = titles,
    Duration = durations,
    Rating = ratings,
    Reviews = reviews,
    Year = years,
    Genre = rep(genre, max_length), # Add genre label here
    stringsAsFactors = FALSE
  )
  
  # Append the temporary data frame to the main data frame
  imdb_data <- bind_rows(imdb_data, temp_data)
}

# Display the combined data with genre labels
print(imdb_data)
```


3.  Clean the data. Next, perform some basic wrangling, such as remove extra whitespace, handle missing values, and convert data types as needed. You might find the functions from `dplyr` or `tidyr` useful for any additional transformations, such as renaming columns, filtering rows, or creating new variables.

convert Duration from to Duration in minutes and Reviews from (39K) to 39000.
```{r}
# Convert Duration and Reviews
imdb_data %>%
  mutate(Reviews = gsub("^\\(|\\)$", "", Reviews)) %>%
  mutate(Reviews = ifelse(grepl("\\.\\d+K$", Reviews),
                          paste0(gsub("\\.", "", sub("K$", "", Reviews)), "00"),
                          Reviews)) %>%
  mutate(Reviews = ifelse(grepl("K$", Reviews),
                          paste0(sub("K$", "", Reviews), "000"),
                          Reviews)) %>%
  mutate(Reviews = ifelse(grepl("\\.\\d+M$", Reviews),
                          paste0(gsub("\\.", "", sub("M$", "", Reviews)), "00000"),
                          Reviews)) %>%
  separate(Duration, into = c("Hours", "Minutes"), sep = "h ", remove = FALSE) %>%
  mutate(Hours = as.numeric(Hours),
         Minutes = as.numeric(sub("m", "", Minutes))) %>%
  separate(Duration, into = c("Hours", "Minutes"), sep = "h ", remove = FALSE) %>%
  mutate(Hours = as.numeric(Hours),
         Minutes = as.numeric(sub("m", "", Minutes)),
         Minutes2 = Hours * 60,
         Duration = Minutes + Minutes2) %>%
  select(-Hours, -Minutes, -Minutes2)
```


4.  Analyze the data. Perform a simple analysis of your choice. For example, you could

    -   Count how many times specific words or themes appear.
    -   Create a summary statistic (e.g., average rating, job salary, team win percentage).
    -   Create a data visualization (e.g., bar chart, histogram) of an interesting metric.

5.  Report your findings. Provide a paragraph summarizing your methods and key findings. Include any limitations or potential biases in your scraping or analysis. Be sure to comment and organize your code so is easy to understand what you are doing.
